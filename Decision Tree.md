## ðŸŒ³ Decision Tree (Complete, Clear & Exam-Ready)

## ðŸ” Definition

**Decision Tree** is a **supervised machine learning algorithm** used for **classification and regression** that makes decisions by **splitting data into branches** based on feature conditions, forming a **tree-like structure**.

ðŸ‘‰ It works exactly like **human decision-making** (ifâ€“else logic).

---

## ðŸ§  Core Idea (One Line)

> A Decision Tree splits data step-by-step using rules until it reaches a final decision (leaf).

---

## ðŸŒ³ Structure of a Decision Tree

* **Root Node** â€“ first split (most important feature)
* **Decision Node** â€“ internal node with a condition
* **Branch** â€“ outcome of a condition
* **Leaf Node** â€“ final prediction

---

## ðŸ§  How Decision Tree Works (Step-by-Step)

1ï¸âƒ£ Select the **best feature** to split the data
2ï¸âƒ£ Split data based on a condition
3ï¸âƒ£ Repeat splitting for child nodes
4ï¸âƒ£ Stop when:

* All samples belong to one class, or
* Maximum depth is reached
  5ï¸âƒ£ Leaf node gives final output

---

## ðŸ“Š How Does It Choose the Best Split?

### ðŸ”¹ 1. Entropy (Information Gain)

#### Entropy

[
Entropy = -\sum p \log_2(p)
]

* Measures **impurity**
* 0 â†’ pure node

#### Information Gain

[
IG = Entropy(parent) - \sum Entropy(children)
]

âœ” Feature with **highest Information Gain** is chosen

---

### ðŸ”¹ 2. Gini Index

[
Gini = 1 - \sum p^2
]

* Used in **CART** trees
* Lower Gini â†’ better split

---

## ðŸ“˜ Types of Decision Trees

### 1ï¸âƒ£ Classification Tree

* Output: Category
* Example: Pass / Fail, Spam / Not Spam

### 2ï¸âƒ£ Regression Tree

* Output: Continuous value
* Example: House price prediction

---

## ðŸ§ª Example

**Loan Approval**

```
Is Income > 5L?
 â”œâ”€ Yes â†’ Is Credit Score good?
 â”‚     â”œâ”€ Yes â†’ Approve
 â”‚     â””â”€ No â†’ Reject
 â””â”€ No â†’ Reject
```

---

## ðŸ“Œ Stopping Criteria

* Maximum tree depth
* Minimum samples per leaf
* No improvement in split

---

## âš ï¸ Overfitting Problem

Decision Trees can **memorize training data**.

### ðŸ”§ Solutions:

* Pruning
* Limiting depth
* Minimum samples per split

---

## ðŸ“Š Performance Metrics

* Accuracy
* Precision / Recall
* RMSE (for regression)

---

## âœ… Advantages

âœ” Easy to understand & visualize
âœ” No feature scaling needed
âœ” Handles both numerical & categorical data
âœ” Non-linear relationships supported

---

## âŒ Disadvantages

âŒ Overfitting
âŒ Unstable (small data change â†’ big tree change)
âŒ Lower accuracy compared to ensemble models

---

## ðŸ§  One-Line Interview Answer

> **Decision Tree is a supervised learning algorithm that makes predictions by recursively splitting data based on feature conditions using a tree-like structure.**

